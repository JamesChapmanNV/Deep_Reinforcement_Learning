{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning: \n",
    "### Deep Q-Networks, Double DQN, Dueling DQN & Prioritized Experience Replay (PER)\n",
    "<br>\n",
    "James Chapman<br>\n",
    "CIS 730 Artificial Intelligence – Term Project<br>\n",
    "Kansas State University"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "This notebook was CONVERTED from OpenAI's Gym to Farama's Gymnasium.\n",
    "\n",
    "This notebook began as part of the tutorial series [\"Deep Reinforcement Learning Explained\"](https://github.com/jorditorresBCN/Deep-Reinforcement-Learning-Explained/tree/master) by [Jordi Torres](https://torres.ai/). Much of Reinforcement learning is not covered in CIS730, and CIS732(Machine Learning) only covered select chapters of \"Reinforcement learning: An introduction\", Sutton & Barto [[1](#citations)]. This series is a great supplement for value iteration and Q-learning in Pytorch.<br>\n",
    "\n",
    "Extending from the series, this project explores 3 variants of Deep Q-Networks, including their application in Breakout.\n",
    "<hr style=\"border:2px solid gray\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "40Yb47zJQglm"
   },
   "source": [
    "DEEP REINFORCEMENT LEARNING EXPLAINED - 15 - 16 - 17\n",
    "# **Deep Q-Network (DQN)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q40Fa7qM4_lE"
   },
   "source": [
    "OpenAI Pong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FA1Y5VCv20XZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "############################\n",
    "#CONVERTED\n",
    "# import gym \n",
    "# import gym.spaces \n",
    "import gymnasium as gym \n",
    "import gymnasium.spaces \n",
    "\n",
    "DEFAULT_ENV_NAME = \"PongNoFrameskip-v4\" \n",
    "test_env = gym.make(DEFAULT_ENV_NAME)\n",
    "print(test_env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8QDaXip14JBv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']\n"
     ]
    }
   ],
   "source": [
    "print(test_env.unwrapped.get_action_meanings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1uzLQLz04z2i"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210, 160, 3)\n"
     ]
    }
   ],
   "source": [
    "print(test_env.observation_space.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZzcdmzIL5EMI"
   },
   "source": [
    "\n",
    "Type of hardware accelerator provided by Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VjUM99rEKFNt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Nov  2 14:19:18 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 536.40                 Driver Version: 536.40       CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                     TCC/WDDM  | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090      WDDM  | 00000000:01:00.0  On |                  Off |\n",
      "|  0%   36C    P8               6W / 450W |    900MiB / 24564MiB |      6%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      8732    C+G   ...5n1h2txyewy\\ShellExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     10084    C+G   C:\\Windows\\explorer.exe                   N/A      |\n",
      "|    0   N/A  N/A     11236    C+G   ...nt.CBS_cw5n1h2txyewy\\SearchHost.exe    N/A      |\n",
      "|    0   N/A  N/A     11252    C+G   ...2txyewy\\StartMenuExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     13248    C+G   ...crosoft\\Edge\\Application\\msedge.exe    N/A      |\n",
      "|    0   N/A  N/A     14344    C+G   ...cal\\Microsoft\\OneDrive\\OneDrive.exe    N/A      |\n",
      "|    0   N/A  N/A     17132    C+G   ...\\iCloud\\WebView2\\msedgewebview2.exe    N/A      |\n",
      "|    0   N/A  N/A     17748    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A     18624    C+G   ...aam7r\\AcrobatNotificationClient.exe    N/A      |\n",
      "|    0   N/A  N/A     18792    C+G   ...on\\118.0.2088.76\\msedgewebview2.exe    N/A      |\n",
      "|    0   N/A  N/A     19920    C+G   ...yj5cx40ttqa\\iCloud\\iCloudPhotos.exe    N/A      |\n",
      "|    0   N/A  N/A     20084    C+G   ...on\\HEX\\Creative Cloud UI Helper.exe    N/A      |\n",
      "|    0   N/A  N/A     20588    C+G   ...oogle\\Chrome\\Application\\chrome.exe    N/A      |\n",
      "|    0   N/A  N/A     23488    C+G   ...siveControlPanel\\SystemSettings.exe    N/A      |\n",
      "|    0   N/A  N/A     24040    C+G   ....5.51.0_x64__htrsf667h5kn2\\AWCC.exe    N/A      |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZhmsqgrHikEl"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pRcuJGVSQi6g"
   },
   "source": [
    "## OpenAI Gym Wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nPi1lHINMuSu"
   },
   "outputs": [],
   "source": [
    "# Taken from \n",
    "# https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On/blob/master/Chapter06/lib/wrappers.py\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "class FireResetEnv(gym.Wrapper):\n",
    "    def __init__(self, env=None):\n",
    "        super(FireResetEnv, self).__init__(env)\n",
    "        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
    "        assert len(env.unwrapped.get_action_meanings()) >= 3\n",
    "\n",
    "    ############################\n",
    "    #CONVERTED\n",
    "    #def step(self, action):\n",
    "    #    return self.env.step(action)\n",
    "    #def reset(self):\n",
    "    #    self.env.reset()\n",
    "    #    obs, _, done, _ = self.env.step(1)\n",
    "    #    if done:\n",
    "    #        self.env.reset()\n",
    "    #    obs, _, done, _ = self.env.step(2)\n",
    "    #    if done:\n",
    "    #        self.env.reset()\n",
    "    #    return obs\n",
    "\n",
    "    def reset(self, **kwargs):#CONVERTED\n",
    "        self.env.reset(**kwargs)\n",
    "        obs, _, terminated, truncated, _ = self.env.step(1)\n",
    "        if terminated or truncated:\n",
    "            self.env.reset(**kwargs)\n",
    "        obs, _, terminated, truncated, _ = self.env.step(2)\n",
    "        if terminated or truncated:\n",
    "            self.env.reset(**kwargs)\n",
    "        return obs, {}\n",
    "\n",
    "class MaxAndSkipEnv(gym.Wrapper):\n",
    "    def __init__(self, env=None, skip=4):\n",
    "        super(MaxAndSkipEnv, self).__init__(env)\n",
    "        # most recent raw observations (for max pooling across time steps)\n",
    "        ############################\n",
    "        #CONVERTED\n",
    "        #self._obs_buffer = collections.deque(maxlen=2)\n",
    "        self._obs_buffer = np.zeros((2, *env.observation_space.shape), dtype=env.observation_space.dtype) #CONVERTED\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        ############################\n",
    "        #CONVERTED\n",
    "        #    total_reward = 0.0\n",
    "        #    done = None\n",
    "        #    for _ in range(self._skip):\n",
    "        #        obs, reward, done, info = self.env.step(action)\n",
    "        #        self._obs_buffer.append(obs)\n",
    "        #        total_reward += reward\n",
    "        #        if done:\n",
    "        #            break\n",
    "        #    max_frame = np.max(np.stack(self._obs_buffer), axis=0)\n",
    "        #    return max_frame, total_reward, done, info\n",
    "        total_reward = 0.0\n",
    "        terminated = truncated = False\n",
    "        for i in range(self._skip):\n",
    "            obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "            done = terminated or truncated\n",
    "            if i == self._skip - 2:\n",
    "                self._obs_buffer[0] = obs\n",
    "            if i == self._skip - 1:\n",
    "                self._obs_buffer[1] = obs\n",
    "            total_reward += float(reward)\n",
    "            if done:\n",
    "                break\n",
    "        max_frame = self._obs_buffer.max(axis=0)\n",
    "\n",
    "        return max_frame, total_reward, terminated, truncated, info\n",
    "    \n",
    "        ############################\n",
    "        #CONVERTED\n",
    "        #def reset(self):\n",
    "        #    self._obs_buffer.clear()\n",
    "        #    obs = self.env.reset()\n",
    "        #    self._obs_buffer.append(obs)\n",
    "        #    return obs\n",
    "\n",
    "\n",
    "class ProcessFrame84(gym.ObservationWrapper):\n",
    "    def __init__(self, env=None):\n",
    "        super(ProcessFrame84, self).__init__(env)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(84, 84, 1), dtype=np.uint8)\n",
    "\n",
    "    def observation(self, obs):\n",
    "        return ProcessFrame84.process(obs)\n",
    "\n",
    "    @staticmethod\n",
    "    def process(frame):\n",
    "        if frame.size == 210 * 160 * 3:\n",
    "            img = np.reshape(frame, [210, 160, 3]).astype(np.float32)\n",
    "        elif frame.size == 250 * 160 * 3:\n",
    "            img = np.reshape(frame, [250, 160, 3]).astype(np.float32)\n",
    "        else:\n",
    "            assert False, \"Unknown resolution.\"\n",
    "        img = img[:, :, 0] * 0.299 + img[:, :, 1] * 0.587 + img[:, :, 2] * 0.114\n",
    "        resized_screen = cv2.resize(img, (84, 110), interpolation=cv2.INTER_AREA)\n",
    "        x_t = resized_screen[18:102, :]\n",
    "        x_t = np.reshape(x_t, [84, 84, 1])\n",
    "        return x_t.astype(np.uint8)\n",
    "\n",
    "\n",
    "class BufferWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env, n_steps, dtype=np.float32):\n",
    "        super(BufferWrapper, self).__init__(env)\n",
    "        self.dtype = dtype\n",
    "        old_space = env.observation_space\n",
    "        self.observation_space = gym.spaces.Box(old_space.low.repeat(n_steps, axis=0),\n",
    "                                                old_space.high.repeat(n_steps, axis=0), dtype=dtype)\n",
    "    ############################\n",
    "    #CONVERTED\n",
    "    #def reset(self):\n",
    "    #    self.buffer = np.zeros_like(self.observation_space.low, dtype=self.dtype)\n",
    "    #    return self.observation(self.env.reset())\n",
    "    def reset(self, **kwargs):#CONVERTED\n",
    "        self.buffer = np.zeros_like(self.observation_space.low, dtype=self.dtype)\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        return self.observation(obs), info\n",
    "\n",
    "    def observation(self, observation):\n",
    "        self.buffer[:-1] = self.buffer[1:]\n",
    "        self.buffer[-1] = observation\n",
    "        return self.buffer\n",
    "\n",
    "\n",
    "class ImageToPyTorch(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(ImageToPyTorch, self).__init__(env)\n",
    "        old_shape = self.observation_space.shape\n",
    "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(old_shape[-1], \n",
    "                                old_shape[0], old_shape[1]), dtype=np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return np.moveaxis(observation, 2, 0)\n",
    "\n",
    "\n",
    "class ScaledFloatFrame(gym.ObservationWrapper):\n",
    "    def observation(self, obs):\n",
    "        return np.array(obs).astype(np.float32) / 255.0\n",
    "\n",
    "def make_env(env_name):\n",
    "    env = gym.make(env_name)\n",
    "    env = MaxAndSkipEnv(env)\n",
    "    env = FireResetEnv(env)\n",
    "    env = ProcessFrame84(env)\n",
    "    env = ImageToPyTorch(env)\n",
    "    env = BufferWrapper(env, 4)\n",
    "    return ScaledFloatFrame(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wznv9I1KR_I3"
   },
   "source": [
    "## The DQN model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h6B8v-Qh5Ykk"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn        # Pytorch neural network package\n",
    "import torch.optim as optim  # Pytorch optimization package\n",
    "\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N4S1I9xWMkf3"
   },
   "outputs": [],
   "source": [
    "# Taken from \n",
    "# https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On/blob/master/Chapter06/lib/dqn_model.py\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "\n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
    "        return self.fc(conv_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 263
    },
    "colab_type": "code",
    "id": "taYi5LZnIOqz",
    "outputId": "c96d3a1d-ebf6-471a-a93f-c96b479cc9fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQN(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
      "    (3): ReLU()\n",
      "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (5): ReLU()\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=3136, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=6, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "test_env = make_env(DEFAULT_ENV_NAME)\n",
    "test_net = DQN(test_env.observation_space.shape, test_env.action_space.n).to(device)\n",
    "print(test_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lhv3Yf-aW7UW"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aPJl73Z1YTa4"
   },
   "source": [
    "Load Tensorboard extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BCBQhXLfNeUG"
   },
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kb_f_onMXkpb"
   },
   "source": [
    "Import required modules and define the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AGwHC9dyXoPd"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "\n",
    "MEAN_REWARD_BOUND = 19.0           \n",
    "\n",
    "gamma = 0.99                   \n",
    "batch_size = 32                \n",
    "replay_size = 10000            \n",
    "learning_rate = 1e-4           \n",
    "sync_target_frames = 1000      \n",
    "replay_start_size = 10000      \n",
    "\n",
    "eps_start=1.0\n",
    "eps_decay=.999985\n",
    "eps_min=0.02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FFaMmDKqYmo4"
   },
   "source": [
    "Experience replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y79CNYsjY4w0"
   },
   "outputs": [],
   "source": [
    "Experience = collections.namedtuple('Experience', field_names=['state', 'action', 'reward', 'done', 'new_state'])\n",
    "\n",
    "class ExperienceReplay:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def append(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "        states, actions, rewards, dones, next_states = zip(*[self.buffer[idx] for idx in indices])\n",
    "        return np.array(states), np.array(actions), np.array(rewards, dtype=np.float32), \\\n",
    "               np.array(dones, dtype=np.uint8), np.array(next_states)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fQDV04ktY3xs"
   },
   "source": [
    "Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YdAKFiMWZw90"
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env, exp_buffer):\n",
    "        self.env = env\n",
    "        self.exp_buffer = exp_buffer\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self):\n",
    "        ############################\n",
    "        #CONVERTED\n",
    "        #self.state = env.reset()\n",
    "        self.state, info = env.reset()\n",
    "        \n",
    "        self.total_reward = 0.0\n",
    "\n",
    "    def play_step(self, net, epsilon=0.0, device=\"cpu\"):\n",
    "\n",
    "        done_reward = None\n",
    "        if np.random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            state_a = np.array([self.state], copy=False)\n",
    "            state_v = torch.tensor(state_a).to(device)\n",
    "            q_vals_v = net(state_v)\n",
    "            _, act_v = torch.max(q_vals_v, dim=1)\n",
    "            action = int(act_v.item())\n",
    "        ############################\n",
    "        #CONVERTED\n",
    "        #new_state, reward, is_done, _ = self.env.step(action)\n",
    "        new_state, reward, terminated, truncated, info = self.env.step(action)\n",
    "        is_done = terminated or truncated  #New\n",
    "        \n",
    "        self.total_reward += reward\n",
    "        \n",
    "        exp = Experience(self.state, action, reward, is_done, new_state)\n",
    "        self.exp_buffer.append(exp)\n",
    "        self.state = new_state\n",
    "        if is_done:\n",
    "            done_reward = self.total_reward\n",
    "            self._reset()\n",
    "        return done_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ipurwYpa6iKn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>Training starts at  2023-11-02 14:19:23.824102\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "print(\">>>Training starts at \",datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bgpmAtchZwM_"
   },
   "source": [
    "Main training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qEoc2PWmM2mu",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "880:  1 games, mean reward -21.000, (epsilon 0.99)\n",
      "Best mean reward updated -21.000\n",
      "1838:  2 games, mean reward -21.000, (epsilon 0.97)\n",
      "2679:  3 games, mean reward -20.667, (epsilon 0.96)\n",
      "Best mean reward updated -20.667\n",
      "3469:  4 games, mean reward -20.750, (epsilon 0.95)\n",
      "4413:  5 games, mean reward -20.800, (epsilon 0.94)\n",
      "5402:  6 games, mean reward -20.833, (epsilon 0.92)\n",
      "6398:  7 games, mean reward -20.714, (epsilon 0.91)\n",
      "7222:  8 games, mean reward -20.750, (epsilon 0.90)\n",
      "8032:  9 games, mean reward -20.778, (epsilon 0.89)\n",
      "8947:  10 games, mean reward -20.700, (epsilon 0.87)\n",
      "10216:  11 games, mean reward -20.455, (epsilon 0.86)\n",
      "Best mean reward updated -20.455\n",
      "10978:  12 games, mean reward -20.500, (epsilon 0.85)\n",
      "11878:  13 games, mean reward -20.462, (epsilon 0.84)\n",
      "12807:  14 games, mean reward -20.429, (epsilon 0.83)\n",
      "Best mean reward updated -20.429\n",
      "13569:  15 games, mean reward -20.467, (epsilon 0.82)\n",
      "14331:  16 games, mean reward -20.500, (epsilon 0.81)\n",
      "15093:  17 games, mean reward -20.529, (epsilon 0.80)\n",
      "15961:  18 games, mean reward -20.556, (epsilon 0.79)\n",
      "16905:  19 games, mean reward -20.579, (epsilon 0.78)\n",
      "17787:  20 games, mean reward -20.600, (epsilon 0.77)\n",
      "18753:  21 games, mean reward -20.571, (epsilon 0.75)\n",
      "19992:  22 games, mean reward -20.500, (epsilon 0.74)\n",
      "20912:  23 games, mean reward -20.478, (epsilon 0.73)\n",
      "21831:  24 games, mean reward -20.417, (epsilon 0.72)\n",
      "Best mean reward updated -20.417\n",
      "22803:  25 games, mean reward -20.440, (epsilon 0.71)\n",
      "23722:  26 games, mean reward -20.385, (epsilon 0.70)\n",
      "Best mean reward updated -20.385\n",
      "24639:  27 games, mean reward -20.370, (epsilon 0.69)\n",
      "Best mean reward updated -20.370\n",
      "25401:  28 games, mean reward -20.393, (epsilon 0.68)\n",
      "26163:  29 games, mean reward -20.414, (epsilon 0.68)\n",
      "27048:  30 games, mean reward -20.400, (epsilon 0.67)\n",
      "28106:  31 games, mean reward -20.355, (epsilon 0.66)\n",
      "Best mean reward updated -20.355\n",
      "29245:  32 games, mean reward -20.312, (epsilon 0.64)\n",
      "Best mean reward updated -20.312\n",
      "30025:  33 games, mean reward -20.333, (epsilon 0.64)\n",
      "30806:  34 games, mean reward -20.353, (epsilon 0.63)\n",
      "31765:  35 games, mean reward -20.371, (epsilon 0.62)\n",
      "32705:  36 games, mean reward -20.389, (epsilon 0.61)\n",
      "33645:  37 games, mean reward -20.405, (epsilon 0.60)\n",
      "34485:  38 games, mean reward -20.395, (epsilon 0.60)\n",
      "35335:  39 games, mean reward -20.410, (epsilon 0.59)\n",
      "36157:  40 games, mean reward -20.425, (epsilon 0.58)\n",
      "37381:  41 games, mean reward -20.415, (epsilon 0.57)\n",
      "38584:  42 games, mean reward -20.405, (epsilon 0.56)\n",
      "39746:  43 games, mean reward -20.372, (epsilon 0.55)\n",
      "41254:  44 games, mean reward -20.318, (epsilon 0.54)\n",
      "42133:  45 games, mean reward -20.333, (epsilon 0.53)\n",
      "43059:  46 games, mean reward -20.326, (epsilon 0.52)\n",
      "44115:  47 games, mean reward -20.319, (epsilon 0.52)\n",
      "45145:  48 games, mean reward -20.333, (epsilon 0.51)\n",
      "46438:  49 games, mean reward -20.306, (epsilon 0.50)\n",
      "Best mean reward updated -20.306\n",
      "47799:  50 games, mean reward -20.240, (epsilon 0.49)\n",
      "Best mean reward updated -20.240\n",
      "49157:  51 games, mean reward -20.235, (epsilon 0.48)\n",
      "Best mean reward updated -20.235\n",
      "50490:  52 games, mean reward -20.231, (epsilon 0.47)\n",
      "Best mean reward updated -20.231\n",
      "51951:  53 games, mean reward -20.189, (epsilon 0.46)\n",
      "Best mean reward updated -20.189\n",
      "53467:  54 games, mean reward -20.148, (epsilon 0.45)\n",
      "Best mean reward updated -20.148\n",
      "55035:  55 games, mean reward -20.091, (epsilon 0.44)\n",
      "Best mean reward updated -20.091\n",
      "56480:  56 games, mean reward -20.071, (epsilon 0.43)\n",
      "Best mean reward updated -20.071\n",
      "57866:  57 games, mean reward -20.053, (epsilon 0.42)\n",
      "Best mean reward updated -20.053\n",
      "59280:  58 games, mean reward -20.000, (epsilon 0.41)\n",
      "Best mean reward updated -20.000\n",
      "60937:  59 games, mean reward -19.932, (epsilon 0.40)\n",
      "Best mean reward updated -19.932\n",
      "62124:  60 games, mean reward -19.917, (epsilon 0.39)\n",
      "Best mean reward updated -19.917\n",
      "63263:  61 games, mean reward -19.902, (epsilon 0.39)\n",
      "Best mean reward updated -19.902\n",
      "64819:  62 games, mean reward -19.887, (epsilon 0.38)\n",
      "Best mean reward updated -19.887\n",
      "66321:  63 games, mean reward -19.841, (epsilon 0.37)\n",
      "Best mean reward updated -19.841\n",
      "67639:  64 games, mean reward -19.812, (epsilon 0.36)\n",
      "Best mean reward updated -19.812\n",
      "68732:  65 games, mean reward -19.831, (epsilon 0.36)\n",
      "70109:  66 games, mean reward -19.803, (epsilon 0.35)\n",
      "Best mean reward updated -19.803\n",
      "71936:  67 games, mean reward -19.791, (epsilon 0.34)\n",
      "Best mean reward updated -19.791\n",
      "73502:  68 games, mean reward -19.794, (epsilon 0.33)\n",
      "75301:  69 games, mean reward -19.710, (epsilon 0.32)\n",
      "Best mean reward updated -19.710\n",
      "76495:  70 games, mean reward -19.714, (epsilon 0.32)\n",
      "77831:  71 games, mean reward -19.718, (epsilon 0.31)\n",
      "79677:  72 games, mean reward -19.681, (epsilon 0.30)\n",
      "Best mean reward updated -19.681\n",
      "81366:  73 games, mean reward -19.644, (epsilon 0.30)\n",
      "Best mean reward updated -19.644\n",
      "82723:  74 games, mean reward -19.622, (epsilon 0.29)\n",
      "Best mean reward updated -19.622\n",
      "84698:  75 games, mean reward -19.560, (epsilon 0.28)\n",
      "Best mean reward updated -19.560\n",
      "86610:  76 games, mean reward -19.539, (epsilon 0.27)\n",
      "Best mean reward updated -19.539\n",
      "88433:  77 games, mean reward -19.519, (epsilon 0.27)\n",
      "Best mean reward updated -19.519\n",
      "90392:  78 games, mean reward -19.487, (epsilon 0.26)\n",
      "Best mean reward updated -19.487\n",
      "92070:  79 games, mean reward -19.481, (epsilon 0.25)\n",
      "Best mean reward updated -19.481\n",
      "93780:  80 games, mean reward -19.475, (epsilon 0.24)\n",
      "Best mean reward updated -19.475\n",
      "95591:  81 games, mean reward -19.444, (epsilon 0.24)\n",
      "Best mean reward updated -19.444\n",
      "97043:  82 games, mean reward -19.427, (epsilon 0.23)\n",
      "Best mean reward updated -19.427\n",
      "98714:  83 games, mean reward -19.398, (epsilon 0.23)\n",
      "Best mean reward updated -19.398\n",
      "100232:  84 games, mean reward -19.381, (epsilon 0.22)\n",
      "Best mean reward updated -19.381\n",
      "101625:  85 games, mean reward -19.353, (epsilon 0.22)\n",
      "Best mean reward updated -19.353\n",
      "103233:  86 games, mean reward -19.360, (epsilon 0.21)\n",
      "105746:  87 games, mean reward -19.264, (epsilon 0.20)\n",
      "Best mean reward updated -19.264\n",
      "107182:  88 games, mean reward -19.273, (epsilon 0.20)\n",
      "108576:  89 games, mean reward -19.281, (epsilon 0.20)\n",
      "110204:  90 games, mean reward -19.256, (epsilon 0.19)\n",
      "Best mean reward updated -19.256\n",
      "111790:  91 games, mean reward -19.253, (epsilon 0.19)\n",
      "Best mean reward updated -19.253\n",
      "113855:  92 games, mean reward -19.196, (epsilon 0.18)\n",
      "Best mean reward updated -19.196\n",
      "116459:  93 games, mean reward -19.129, (epsilon 0.17)\n",
      "Best mean reward updated -19.129\n",
      "118298:  94 games, mean reward -19.096, (epsilon 0.17)\n",
      "Best mean reward updated -19.096\n",
      "119694:  95 games, mean reward -19.095, (epsilon 0.17)\n",
      "Best mean reward updated -19.095\n",
      "121332:  96 games, mean reward -19.073, (epsilon 0.16)\n",
      "Best mean reward updated -19.073\n",
      "122955:  97 games, mean reward -19.072, (epsilon 0.16)\n",
      "Best mean reward updated -19.072\n",
      "125239:  98 games, mean reward -19.020, (epsilon 0.15)\n",
      "Best mean reward updated -19.020\n",
      "126941:  99 games, mean reward -19.020, (epsilon 0.15)\n",
      "Best mean reward updated -19.020\n",
      "129324:  100 games, mean reward -18.980, (epsilon 0.14)\n",
      "Best mean reward updated -18.980\n",
      "131177:  101 games, mean reward -18.960, (epsilon 0.14)\n",
      "Best mean reward updated -18.960\n",
      "133518:  102 games, mean reward -18.900, (epsilon 0.13)\n",
      "Best mean reward updated -18.900\n",
      "135846:  103 games, mean reward -18.800, (epsilon 0.13)\n",
      "Best mean reward updated -18.800\n",
      "138306:  104 games, mean reward -18.740, (epsilon 0.13)\n",
      "Best mean reward updated -18.740\n",
      "140724:  105 games, mean reward -18.690, (epsilon 0.12)\n",
      "Best mean reward updated -18.690\n",
      "143608:  106 games, mean reward -18.610, (epsilon 0.12)\n",
      "Best mean reward updated -18.610\n",
      "145897:  107 games, mean reward -18.560, (epsilon 0.11)\n",
      "Best mean reward updated -18.560\n",
      "148092:  108 games, mean reward -18.530, (epsilon 0.11)\n",
      "Best mean reward updated -18.530\n",
      "150378:  109 games, mean reward -18.480, (epsilon 0.10)\n",
      "Best mean reward updated -18.480\n",
      "153255:  110 games, mean reward -18.360, (epsilon 0.10)\n",
      "Best mean reward updated -18.360\n",
      "155487:  111 games, mean reward -18.320, (epsilon 0.10)\n",
      "Best mean reward updated -18.320\n",
      "157701:  112 games, mean reward -18.270, (epsilon 0.09)\n",
      "Best mean reward updated -18.270\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160326:  113 games, mean reward -18.200, (epsilon 0.09)\n",
      "Best mean reward updated -18.200\n",
      "162986:  114 games, mean reward -18.160, (epsilon 0.09)\n",
      "Best mean reward updated -18.160\n",
      "165687:  115 games, mean reward -18.080, (epsilon 0.08)\n",
      "Best mean reward updated -18.080\n",
      "168060:  116 games, mean reward -18.050, (epsilon 0.08)\n",
      "Best mean reward updated -18.050\n",
      "170270:  117 games, mean reward -17.970, (epsilon 0.08)\n",
      "Best mean reward updated -17.970\n",
      "172954:  118 games, mean reward -17.860, (epsilon 0.07)\n",
      "Best mean reward updated -17.860\n",
      "175062:  119 games, mean reward -17.810, (epsilon 0.07)\n",
      "Best mean reward updated -17.810\n",
      "177079:  120 games, mean reward -17.740, (epsilon 0.07)\n",
      "Best mean reward updated -17.740\n",
      "179872:  121 games, mean reward -17.630, (epsilon 0.07)\n",
      "Best mean reward updated -17.630\n",
      "182430:  122 games, mean reward -17.590, (epsilon 0.06)\n",
      "Best mean reward updated -17.590\n",
      "185276:  123 games, mean reward -17.480, (epsilon 0.06)\n",
      "Best mean reward updated -17.480\n",
      "187710:  124 games, mean reward -17.410, (epsilon 0.06)\n",
      "Best mean reward updated -17.410\n",
      "190734:  125 games, mean reward -17.310, (epsilon 0.06)\n",
      "Best mean reward updated -17.310\n",
      "193860:  126 games, mean reward -17.170, (epsilon 0.05)\n",
      "Best mean reward updated -17.170\n",
      "196364:  127 games, mean reward -17.080, (epsilon 0.05)\n",
      "Best mean reward updated -17.080\n",
      "199044:  128 games, mean reward -16.940, (epsilon 0.05)\n",
      "Best mean reward updated -16.940\n",
      "202920:  129 games, mean reward -16.780, (epsilon 0.05)\n",
      "Best mean reward updated -16.780\n",
      "206710:  130 games, mean reward -16.630, (epsilon 0.05)\n",
      "Best mean reward updated -16.630\n",
      "210411:  131 games, mean reward -16.470, (epsilon 0.04)\n",
      "Best mean reward updated -16.470\n",
      "214448:  132 games, mean reward -16.270, (epsilon 0.04)\n",
      "Best mean reward updated -16.270\n",
      "218421:  133 games, mean reward -16.030, (epsilon 0.04)\n",
      "Best mean reward updated -16.030\n",
      "221063:  134 games, mean reward -15.920, (epsilon 0.04)\n",
      "Best mean reward updated -15.920\n",
      "225337:  135 games, mean reward -15.740, (epsilon 0.03)\n",
      "Best mean reward updated -15.740\n",
      "228660:  136 games, mean reward -15.580, (epsilon 0.03)\n",
      "Best mean reward updated -15.580\n",
      "232222:  137 games, mean reward -15.320, (epsilon 0.03)\n",
      "Best mean reward updated -15.320\n",
      "235889:  138 games, mean reward -15.090, (epsilon 0.03)\n",
      "Best mean reward updated -15.090\n",
      "240535:  139 games, mean reward -14.850, (epsilon 0.03)\n",
      "Best mean reward updated -14.850\n",
      "244664:  140 games, mean reward -14.670, (epsilon 0.03)\n",
      "Best mean reward updated -14.670\n",
      "247901:  141 games, mean reward -14.520, (epsilon 0.02)\n",
      "Best mean reward updated -14.520\n",
      "252077:  142 games, mean reward -14.340, (epsilon 0.02)\n",
      "Best mean reward updated -14.340\n",
      "256255:  143 games, mean reward -14.160, (epsilon 0.02)\n",
      "Best mean reward updated -14.160\n",
      "259584:  144 games, mean reward -14.000, (epsilon 0.02)\n",
      "Best mean reward updated -14.000\n",
      "263350:  145 games, mean reward -13.820, (epsilon 0.02)\n",
      "Best mean reward updated -13.820\n",
      "266657:  146 games, mean reward -13.560, (epsilon 0.02)\n",
      "Best mean reward updated -13.560\n",
      "270474:  147 games, mean reward -13.340, (epsilon 0.02)\n",
      "Best mean reward updated -13.340\n",
      "274287:  148 games, mean reward -13.080, (epsilon 0.02)\n",
      "Best mean reward updated -13.080\n",
      "277779:  149 games, mean reward -12.810, (epsilon 0.02)\n",
      "Best mean reward updated -12.810\n",
      "280660:  150 games, mean reward -12.500, (epsilon 0.02)\n",
      "Best mean reward updated -12.500\n",
      "283976:  151 games, mean reward -12.200, (epsilon 0.02)\n",
      "Best mean reward updated -12.200\n",
      "286737:  152 games, mean reward -11.900, (epsilon 0.02)\n",
      "Best mean reward updated -11.900\n",
      "290009:  153 games, mean reward -11.610, (epsilon 0.02)\n",
      "Best mean reward updated -11.610\n",
      "293038:  154 games, mean reward -11.300, (epsilon 0.02)\n",
      "Best mean reward updated -11.300\n",
      "296378:  155 games, mean reward -11.040, (epsilon 0.02)\n",
      "Best mean reward updated -11.040\n",
      "299583:  156 games, mean reward -10.740, (epsilon 0.02)\n",
      "Best mean reward updated -10.740\n",
      "302857:  157 games, mean reward -10.440, (epsilon 0.02)\n",
      "Best mean reward updated -10.440\n",
      "305643:  158 games, mean reward -10.140, (epsilon 0.02)\n",
      "Best mean reward updated -10.140\n",
      "308423:  159 games, mean reward -9.860, (epsilon 0.02)\n",
      "Best mean reward updated -9.860\n",
      "311395:  160 games, mean reward -9.550, (epsilon 0.02)\n",
      "Best mean reward updated -9.550\n",
      "313994:  161 games, mean reward -9.220, (epsilon 0.02)\n",
      "Best mean reward updated -9.220\n",
      "316791:  162 games, mean reward -8.900, (epsilon 0.02)\n",
      "Best mean reward updated -8.900\n",
      "320092:  163 games, mean reward -8.650, (epsilon 0.02)\n",
      "Best mean reward updated -8.650\n",
      "323192:  164 games, mean reward -8.340, (epsilon 0.02)\n",
      "Best mean reward updated -8.340\n",
      "326002:  165 games, mean reward -8.020, (epsilon 0.02)\n",
      "Best mean reward updated -8.020\n",
      "328514:  166 games, mean reward -7.700, (epsilon 0.02)\n",
      "Best mean reward updated -7.700\n",
      "331595:  167 games, mean reward -7.400, (epsilon 0.02)\n",
      "Best mean reward updated -7.400\n",
      "334422:  168 games, mean reward -7.070, (epsilon 0.02)\n",
      "Best mean reward updated -7.070\n",
      "337608:  169 games, mean reward -6.850, (epsilon 0.02)\n",
      "Best mean reward updated -6.850\n",
      "340887:  170 games, mean reward -6.540, (epsilon 0.02)\n",
      "Best mean reward updated -6.540\n",
      "343537:  171 games, mean reward -6.210, (epsilon 0.02)\n",
      "Best mean reward updated -6.210\n",
      "346341:  172 games, mean reward -5.920, (epsilon 0.02)\n",
      "Best mean reward updated -5.920\n",
      "348981:  173 games, mean reward -5.590, (epsilon 0.02)\n",
      "Best mean reward updated -5.590\n",
      "351796:  174 games, mean reward -5.300, (epsilon 0.02)\n",
      "Best mean reward updated -5.300\n",
      "354577:  175 games, mean reward -5.030, (epsilon 0.02)\n",
      "Best mean reward updated -5.030\n",
      "357079:  176 games, mean reward -4.720, (epsilon 0.02)\n",
      "Best mean reward updated -4.720\n",
      "359867:  177 games, mean reward -4.450, (epsilon 0.02)\n",
      "Best mean reward updated -4.450\n",
      "361945:  178 games, mean reward -4.090, (epsilon 0.02)\n",
      "Best mean reward updated -4.090\n",
      "364454:  179 games, mean reward -3.770, (epsilon 0.02)\n",
      "Best mean reward updated -3.770\n",
      "366633:  180 games, mean reward -3.420, (epsilon 0.02)\n",
      "Best mean reward updated -3.420\n",
      "369393:  181 games, mean reward -3.120, (epsilon 0.02)\n",
      "Best mean reward updated -3.120\n",
      "371953:  182 games, mean reward -2.850, (epsilon 0.02)\n",
      "Best mean reward updated -2.850\n",
      "374229:  183 games, mean reward -2.510, (epsilon 0.02)\n",
      "Best mean reward updated -2.510\n",
      "376356:  184 games, mean reward -2.170, (epsilon 0.02)\n",
      "Best mean reward updated -2.170\n",
      "378401:  185 games, mean reward -1.810, (epsilon 0.02)\n",
      "Best mean reward updated -1.810\n",
      "380461:  186 games, mean reward -1.430, (epsilon 0.02)\n",
      "Best mean reward updated -1.430\n",
      "382624:  187 games, mean reward -1.160, (epsilon 0.02)\n",
      "Best mean reward updated -1.160\n",
      "384547:  188 games, mean reward -0.770, (epsilon 0.02)\n",
      "Best mean reward updated -0.770\n",
      "386626:  189 games, mean reward -0.390, (epsilon 0.02)\n",
      "Best mean reward updated -0.390\n",
      "388478:  190 games, mean reward -0.020, (epsilon 0.02)\n",
      "Best mean reward updated -0.020\n",
      "390713:  191 games, mean reward 0.330, (epsilon 0.02)\n",
      "Best mean reward updated 0.330\n",
      "392494:  192 games, mean reward 0.670, (epsilon 0.02)\n",
      "Best mean reward updated 0.670\n",
      "394565:  193 games, mean reward 0.960, (epsilon 0.02)\n",
      "Best mean reward updated 0.960\n",
      "396430:  194 games, mean reward 1.310, (epsilon 0.02)\n",
      "Best mean reward updated 1.310\n",
      "398380:  195 games, mean reward 1.670, (epsilon 0.02)\n",
      "Best mean reward updated 1.670\n",
      "400173:  196 games, mean reward 2.020, (epsilon 0.02)\n",
      "Best mean reward updated 2.020\n",
      "401988:  197 games, mean reward 2.400, (epsilon 0.02)\n",
      "Best mean reward updated 2.400\n",
      "403660:  198 games, mean reward 2.740, (epsilon 0.02)\n",
      "Best mean reward updated 2.740\n",
      "405821:  199 games, mean reward 3.090, (epsilon 0.02)\n",
      "Best mean reward updated 3.090\n",
      "407640:  200 games, mean reward 3.440, (epsilon 0.02)\n",
      "Best mean reward updated 3.440\n",
      "409708:  201 games, mean reward 3.800, (epsilon 0.02)\n",
      "Best mean reward updated 3.800\n",
      "411962:  202 games, mean reward 4.100, (epsilon 0.02)\n",
      "Best mean reward updated 4.100\n",
      "413970:  203 games, mean reward 4.390, (epsilon 0.02)\n",
      "Best mean reward updated 4.390\n",
      "415885:  204 games, mean reward 4.740, (epsilon 0.02)\n",
      "Best mean reward updated 4.740\n",
      "417922:  205 games, mean reward 5.080, (epsilon 0.02)\n",
      "Best mean reward updated 5.080\n",
      "420379:  206 games, mean reward 5.310, (epsilon 0.02)\n",
      "Best mean reward updated 5.310\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "422050:  207 games, mean reward 5.660, (epsilon 0.02)\n",
      "Best mean reward updated 5.660\n",
      "423880:  208 games, mean reward 6.030, (epsilon 0.02)\n",
      "Best mean reward updated 6.030\n",
      "425656:  209 games, mean reward 6.390, (epsilon 0.02)\n",
      "Best mean reward updated 6.390\n",
      "427553:  210 games, mean reward 6.670, (epsilon 0.02)\n",
      "Best mean reward updated 6.670\n",
      "430197:  211 games, mean reward 6.930, (epsilon 0.02)\n",
      "Best mean reward updated 6.930\n",
      "432218:  212 games, mean reward 7.250, (epsilon 0.02)\n",
      "Best mean reward updated 7.250\n",
      "433920:  213 games, mean reward 7.590, (epsilon 0.02)\n",
      "Best mean reward updated 7.590\n",
      "435747:  214 games, mean reward 7.930, (epsilon 0.02)\n",
      "Best mean reward updated 7.930\n",
      "438044:  215 games, mean reward 8.240, (epsilon 0.02)\n",
      "Best mean reward updated 8.240\n",
      "439858:  216 games, mean reward 8.600, (epsilon 0.02)\n",
      "Best mean reward updated 8.600\n",
      "442053:  217 games, mean reward 8.900, (epsilon 0.02)\n",
      "Best mean reward updated 8.900\n",
      "443862:  218 games, mean reward 9.200, (epsilon 0.02)\n",
      "Best mean reward updated 9.200\n",
      "445566:  219 games, mean reward 9.560, (epsilon 0.02)\n",
      "Best mean reward updated 9.560\n",
      "447300:  220 games, mean reward 9.890, (epsilon 0.02)\n",
      "Best mean reward updated 9.890\n",
      "449351:  221 games, mean reward 10.140, (epsilon 0.02)\n",
      "Best mean reward updated 10.140\n",
      "451052:  222 games, mean reward 10.500, (epsilon 0.02)\n",
      "Best mean reward updated 10.500\n",
      "452952:  223 games, mean reward 10.780, (epsilon 0.02)\n",
      "Best mean reward updated 10.780\n",
      "454935:  224 games, mean reward 11.070, (epsilon 0.02)\n",
      "Best mean reward updated 11.070\n",
      "456803:  225 games, mean reward 11.370, (epsilon 0.02)\n",
      "Best mean reward updated 11.370\n",
      "458998:  226 games, mean reward 11.590, (epsilon 0.02)\n",
      "Best mean reward updated 11.590\n",
      "460877:  227 games, mean reward 11.880, (epsilon 0.02)\n",
      "Best mean reward updated 11.880\n",
      "462793:  228 games, mean reward 12.130, (epsilon 0.02)\n",
      "Best mean reward updated 12.130\n",
      "464689:  229 games, mean reward 12.360, (epsilon 0.02)\n",
      "Best mean reward updated 12.360\n",
      "466592:  230 games, mean reward 12.600, (epsilon 0.02)\n",
      "Best mean reward updated 12.600\n",
      "468838:  231 games, mean reward 12.790, (epsilon 0.02)\n",
      "Best mean reward updated 12.790\n",
      "470971:  232 games, mean reward 12.950, (epsilon 0.02)\n",
      "Best mean reward updated 12.950\n",
      "472866:  233 games, mean reward 13.130, (epsilon 0.02)\n",
      "Best mean reward updated 13.130\n",
      "473780:  234 games, mean reward 13.040, (epsilon 0.02)\n",
      "475510:  235 games, mean reward 13.260, (epsilon 0.02)\n",
      "Best mean reward updated 13.260\n",
      "477241:  236 games, mean reward 13.510, (epsilon 0.02)\n",
      "Best mean reward updated 13.510\n",
      "479270:  237 games, mean reward 13.630, (epsilon 0.02)\n",
      "Best mean reward updated 13.630\n",
      "481056:  238 games, mean reward 13.790, (epsilon 0.02)\n",
      "Best mean reward updated 13.790\n",
      "482804:  239 games, mean reward 13.960, (epsilon 0.02)\n",
      "Best mean reward updated 13.960\n",
      "484771:  240 games, mean reward 14.170, (epsilon 0.02)\n",
      "Best mean reward updated 14.170\n",
      "486557:  241 games, mean reward 14.400, (epsilon 0.02)\n",
      "Best mean reward updated 14.400\n",
      "488597:  242 games, mean reward 14.590, (epsilon 0.02)\n",
      "Best mean reward updated 14.590\n",
      "490351:  243 games, mean reward 14.800, (epsilon 0.02)\n",
      "Best mean reward updated 14.800\n",
      "492157:  244 games, mean reward 15.010, (epsilon 0.02)\n",
      "Best mean reward updated 15.010\n",
      "494088:  245 games, mean reward 15.170, (epsilon 0.02)\n",
      "Best mean reward updated 15.170\n",
      "496474:  246 games, mean reward 15.190, (epsilon 0.02)\n",
      "Best mean reward updated 15.190\n",
      "498174:  247 games, mean reward 15.380, (epsilon 0.02)\n",
      "Best mean reward updated 15.380\n",
      "500006:  248 games, mean reward 15.520, (epsilon 0.02)\n",
      "Best mean reward updated 15.520\n",
      "501822:  249 games, mean reward 15.630, (epsilon 0.02)\n",
      "Best mean reward updated 15.630\n",
      "503769:  250 games, mean reward 15.670, (epsilon 0.02)\n",
      "Best mean reward updated 15.670\n",
      "505561:  251 games, mean reward 15.770, (epsilon 0.02)\n",
      "Best mean reward updated 15.770\n",
      "507707:  252 games, mean reward 15.780, (epsilon 0.02)\n",
      "Best mean reward updated 15.780\n",
      "509492:  253 games, mean reward 15.850, (epsilon 0.02)\n",
      "Best mean reward updated 15.850\n",
      "511240:  254 games, mean reward 15.920, (epsilon 0.02)\n",
      "Best mean reward updated 15.920\n",
      "512908:  255 games, mean reward 16.030, (epsilon 0.02)\n",
      "Best mean reward updated 16.030\n",
      "514774:  256 games, mean reward 16.110, (epsilon 0.02)\n",
      "Best mean reward updated 16.110\n",
      "516431:  257 games, mean reward 16.210, (epsilon 0.02)\n",
      "Best mean reward updated 16.210\n",
      "518396:  258 games, mean reward 16.260, (epsilon 0.02)\n",
      "Best mean reward updated 16.260\n",
      "520324:  259 games, mean reward 16.310, (epsilon 0.02)\n",
      "Best mean reward updated 16.310\n",
      "521981:  260 games, mean reward 16.400, (epsilon 0.02)\n",
      "Best mean reward updated 16.400\n",
      "523839:  261 games, mean reward 16.450, (epsilon 0.02)\n",
      "Best mean reward updated 16.450\n",
      "525496:  262 games, mean reward 16.530, (epsilon 0.02)\n",
      "Best mean reward updated 16.530\n",
      "527257:  263 games, mean reward 16.640, (epsilon 0.02)\n",
      "Best mean reward updated 16.640\n",
      "529069:  264 games, mean reward 16.700, (epsilon 0.02)\n",
      "Best mean reward updated 16.700\n",
      "531071:  265 games, mean reward 16.760, (epsilon 0.02)\n",
      "Best mean reward updated 16.760\n",
      "532867:  266 games, mean reward 16.810, (epsilon 0.02)\n",
      "Best mean reward updated 16.810\n",
      "534580:  267 games, mean reward 16.900, (epsilon 0.02)\n",
      "Best mean reward updated 16.900\n",
      "536378:  268 games, mean reward 16.970, (epsilon 0.02)\n",
      "Best mean reward updated 16.970\n",
      "538243:  269 games, mean reward 17.080, (epsilon 0.02)\n",
      "Best mean reward updated 17.080\n",
      "540127:  270 games, mean reward 17.160, (epsilon 0.02)\n",
      "Best mean reward updated 17.160\n",
      "541822:  271 games, mean reward 17.240, (epsilon 0.02)\n",
      "Best mean reward updated 17.240\n",
      "543518:  272 games, mean reward 17.330, (epsilon 0.02)\n",
      "Best mean reward updated 17.330\n",
      "545803:  273 games, mean reward 17.330, (epsilon 0.02)\n",
      "547645:  274 games, mean reward 17.410, (epsilon 0.02)\n",
      "Best mean reward updated 17.410\n",
      "549298:  275 games, mean reward 17.500, (epsilon 0.02)\n",
      "Best mean reward updated 17.500\n",
      "551143:  276 games, mean reward 17.570, (epsilon 0.02)\n",
      "Best mean reward updated 17.570\n",
      "552795:  277 games, mean reward 17.690, (epsilon 0.02)\n",
      "Best mean reward updated 17.690\n",
      "554635:  278 games, mean reward 17.690, (epsilon 0.02)\n",
      "556521:  279 games, mean reward 17.740, (epsilon 0.02)\n",
      "Best mean reward updated 17.740\n",
      "558424:  280 games, mean reward 17.770, (epsilon 0.02)\n",
      "Best mean reward updated 17.770\n",
      "560122:  281 games, mean reward 17.850, (epsilon 0.02)\n",
      "Best mean reward updated 17.850\n",
      "561880:  282 games, mean reward 17.960, (epsilon 0.02)\n",
      "Best mean reward updated 17.960\n",
      "563533:  283 games, mean reward 18.000, (epsilon 0.02)\n",
      "Best mean reward updated 18.000\n",
      "565444:  284 games, mean reward 18.020, (epsilon 0.02)\n",
      "Best mean reward updated 18.020\n",
      "567249:  285 games, mean reward 18.010, (epsilon 0.02)\n",
      "568915:  286 games, mean reward 18.030, (epsilon 0.02)\n",
      "Best mean reward updated 18.030\n",
      "570611:  287 games, mean reward 18.070, (epsilon 0.02)\n",
      "Best mean reward updated 18.070\n",
      "573067:  288 games, mean reward 18.030, (epsilon 0.02)\n",
      "574763:  289 games, mean reward 18.060, (epsilon 0.02)\n",
      "576599:  290 games, mean reward 18.050, (epsilon 0.02)\n",
      "578347:  291 games, mean reward 18.080, (epsilon 0.02)\n",
      "Best mean reward updated 18.080\n",
      "580014:  292 games, mean reward 18.080, (epsilon 0.02)\n",
      "582055:  293 games, mean reward 18.070, (epsilon 0.02)\n",
      "583893:  294 games, mean reward 18.060, (epsilon 0.02)\n",
      "585638:  295 games, mean reward 18.090, (epsilon 0.02)\n",
      "Best mean reward updated 18.090\n",
      "587418:  296 games, mean reward 18.110, (epsilon 0.02)\n",
      "Best mean reward updated 18.110\n",
      "589406:  297 games, mean reward 18.100, (epsilon 0.02)\n",
      "591331:  298 games, mean reward 18.090, (epsilon 0.02)\n",
      "593410:  299 games, mean reward 18.090, (epsilon 0.02)\n",
      "595712:  300 games, mean reward 17.960, (epsilon 0.02)\n",
      "597713:  301 games, mean reward 17.980, (epsilon 0.02)\n",
      "599611:  302 games, mean reward 18.020, (epsilon 0.02)\n",
      "601449:  303 games, mean reward 18.020, (epsilon 0.02)\n",
      "603190:  304 games, mean reward 18.020, (epsilon 0.02)\n",
      "605004:  305 games, mean reward 18.030, (epsilon 0.02)\n",
      "606712:  306 games, mean reward 18.130, (epsilon 0.02)\n",
      "Best mean reward updated 18.130\n",
      "608533:  307 games, mean reward 18.110, (epsilon 0.02)\n",
      "610363:  308 games, mean reward 18.090, (epsilon 0.02)\n",
      "612102:  309 games, mean reward 18.090, (epsilon 0.02)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "613992:  310 games, mean reward 18.070, (epsilon 0.02)\n",
      "616116:  311 games, mean reward 18.080, (epsilon 0.02)\n",
      "618190:  312 games, mean reward 18.080, (epsilon 0.02)\n",
      "620148:  313 games, mean reward 18.060, (epsilon 0.02)\n",
      "621804:  314 games, mean reward 18.090, (epsilon 0.02)\n",
      "623659:  315 games, mean reward 18.110, (epsilon 0.02)\n",
      "625589:  316 games, mean reward 18.120, (epsilon 0.02)\n",
      "627285:  317 games, mean reward 18.160, (epsilon 0.02)\n",
      "Best mean reward updated 18.160\n",
      "629333:  318 games, mean reward 18.110, (epsilon 0.02)\n",
      "631088:  319 games, mean reward 18.110, (epsilon 0.02)\n",
      "633007:  320 games, mean reward 18.090, (epsilon 0.02)\n",
      "634931:  321 games, mean reward 18.110, (epsilon 0.02)\n",
      "636945:  322 games, mean reward 18.060, (epsilon 0.02)\n",
      "639028:  323 games, mean reward 18.040, (epsilon 0.02)\n",
      "640848:  324 games, mean reward 18.060, (epsilon 0.02)\n",
      "642683:  325 games, mean reward 18.050, (epsilon 0.02)\n",
      "644381:  326 games, mean reward 18.090, (epsilon 0.02)\n",
      "646339:  327 games, mean reward 18.080, (epsilon 0.02)\n",
      "648109:  328 games, mean reward 18.100, (epsilon 0.02)\n",
      "649889:  329 games, mean reward 18.110, (epsilon 0.02)\n",
      "651616:  330 games, mean reward 18.110, (epsilon 0.02)\n",
      "653273:  331 games, mean reward 18.160, (epsilon 0.02)\n",
      "655187:  332 games, mean reward 18.170, (epsilon 0.02)\n",
      "Best mean reward updated 18.170\n",
      "656911:  333 games, mean reward 18.160, (epsilon 0.02)\n",
      "658771:  334 games, mean reward 18.540, (epsilon 0.02)\n",
      "Best mean reward updated 18.540\n",
      "660467:  335 games, mean reward 18.550, (epsilon 0.02)\n",
      "Best mean reward updated 18.550\n",
      "662480:  336 games, mean reward 18.520, (epsilon 0.02)\n",
      "664347:  337 games, mean reward 18.530, (epsilon 0.02)\n",
      "666085:  338 games, mean reward 18.540, (epsilon 0.02)\n",
      "667872:  339 games, mean reward 18.540, (epsilon 0.02)\n",
      "669751:  340 games, mean reward 18.540, (epsilon 0.02)\n",
      "671668:  341 games, mean reward 18.550, (epsilon 0.02)\n",
      "673665:  342 games, mean reward 18.540, (epsilon 0.02)\n",
      "675631:  343 games, mean reward 18.520, (epsilon 0.02)\n",
      "677287:  344 games, mean reward 18.540, (epsilon 0.02)\n",
      "679038:  345 games, mean reward 18.610, (epsilon 0.02)\n",
      "Best mean reward updated 18.610\n",
      "680693:  346 games, mean reward 18.740, (epsilon 0.02)\n",
      "Best mean reward updated 18.740\n",
      "682484:  347 games, mean reward 18.730, (epsilon 0.02)\n",
      "684224:  348 games, mean reward 18.740, (epsilon 0.02)\n",
      "685929:  349 games, mean reward 18.740, (epsilon 0.02)\n",
      "687675:  350 games, mean reward 18.760, (epsilon 0.02)\n",
      "Best mean reward updated 18.760\n",
      "689342:  351 games, mean reward 18.760, (epsilon 0.02)\n",
      "691194:  352 games, mean reward 18.850, (epsilon 0.02)\n",
      "Best mean reward updated 18.850\n",
      "692845:  353 games, mean reward 18.880, (epsilon 0.02)\n",
      "Best mean reward updated 18.880\n",
      "694626:  354 games, mean reward 18.870, (epsilon 0.02)\n",
      "696673:  355 games, mean reward 18.860, (epsilon 0.02)\n",
      "698460:  356 games, mean reward 18.870, (epsilon 0.02)\n",
      "700337:  357 games, mean reward 18.850, (epsilon 0.02)\n",
      "702283:  358 games, mean reward 18.830, (epsilon 0.02)\n",
      "703982:  359 games, mean reward 18.870, (epsilon 0.02)\n",
      "705699:  360 games, mean reward 18.870, (epsilon 0.02)\n",
      "707353:  361 games, mean reward 18.890, (epsilon 0.02)\n",
      "Best mean reward updated 18.890\n",
      "709005:  362 games, mean reward 18.890, (epsilon 0.02)\n",
      "710905:  363 games, mean reward 18.880, (epsilon 0.02)\n",
      "712808:  364 games, mean reward 18.860, (epsilon 0.02)\n",
      "714460:  365 games, mean reward 18.900, (epsilon 0.02)\n",
      "Best mean reward updated 18.900\n",
      "716220:  366 games, mean reward 18.910, (epsilon 0.02)\n",
      "Best mean reward updated 18.910\n",
      "717919:  367 games, mean reward 18.920, (epsilon 0.02)\n",
      "Best mean reward updated 18.920\n",
      "719588:  368 games, mean reward 18.920, (epsilon 0.02)\n",
      "721455:  369 games, mean reward 18.910, (epsilon 0.02)\n",
      "723272:  370 games, mean reward 18.910, (epsilon 0.02)\n",
      "725292:  371 games, mean reward 18.880, (epsilon 0.02)\n",
      "727133:  372 games, mean reward 18.870, (epsilon 0.02)\n",
      "729052:  373 games, mean reward 18.870, (epsilon 0.02)\n",
      "730702:  374 games, mean reward 18.890, (epsilon 0.02)\n",
      "732641:  375 games, mean reward 18.870, (epsilon 0.02)\n",
      "734391:  376 games, mean reward 18.870, (epsilon 0.02)\n",
      "736106:  377 games, mean reward 18.860, (epsilon 0.02)\n",
      "737758:  378 games, mean reward 18.880, (epsilon 0.02)\n",
      "739454:  379 games, mean reward 18.910, (epsilon 0.02)\n",
      "741272:  380 games, mean reward 18.910, (epsilon 0.02)\n",
      "742925:  381 games, mean reward 18.910, (epsilon 0.02)\n",
      "744689:  382 games, mean reward 18.900, (epsilon 0.02)\n",
      "746389:  383 games, mean reward 18.900, (epsilon 0.02)\n",
      "748195:  384 games, mean reward 18.900, (epsilon 0.02)\n",
      "749951:  385 games, mean reward 18.910, (epsilon 0.02)\n",
      "751602:  386 games, mean reward 18.920, (epsilon 0.02)\n",
      "753304:  387 games, mean reward 18.920, (epsilon 0.02)\n",
      "754957:  388 games, mean reward 18.980, (epsilon 0.02)\n",
      "Best mean reward updated 18.980\n",
      "756607:  389 games, mean reward 18.980, (epsilon 0.02)\n",
      "758360:  390 games, mean reward 18.990, (epsilon 0.02)\n",
      "Best mean reward updated 18.990\n",
      "760229:  391 games, mean reward 18.980, (epsilon 0.02)\n",
      "762008:  392 games, mean reward 18.980, (epsilon 0.02)\n",
      "763834:  393 games, mean reward 19.030, (epsilon 0.02)\n",
      "Best mean reward updated 19.030\n",
      "Solved in 763834 frames!\n"
     ]
    }
   ],
   "source": [
    "env = make_env(DEFAULT_ENV_NAME)\n",
    "\n",
    "net = DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    "target_net = DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    "writer = SummaryWriter(comment=\"-\" + DEFAULT_ENV_NAME)\n",
    " \n",
    "buffer = ExperienceReplay(replay_size)\n",
    "agent = Agent(env, buffer)\n",
    "\n",
    "epsilon = eps_start\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "total_rewards = []\n",
    "frame_idx = 0  \n",
    "\n",
    "best_mean_reward = None\n",
    "\n",
    "while True:\n",
    "        frame_idx += 1\n",
    "        epsilon = max(epsilon*eps_decay, eps_min)\n",
    "\n",
    "        reward = agent.play_step(net, epsilon, device=device)\n",
    "        if reward is not None:\n",
    "            total_rewards.append(reward)\n",
    "\n",
    "            mean_reward = np.mean(total_rewards[-100:])\n",
    "\n",
    "            print(\"%d:  %d games, mean reward %.3f, (epsilon %.2f)\" % (\n",
    "                frame_idx, len(total_rewards), mean_reward, epsilon))\n",
    "            \n",
    "            writer.add_scalar(\"epsilon\", epsilon, frame_idx)\n",
    "            writer.add_scalar(\"reward_100\", mean_reward, frame_idx)\n",
    "            writer.add_scalar(\"reward\", reward, frame_idx)\n",
    "\n",
    "            if best_mean_reward is None or best_mean_reward < mean_reward:\n",
    "                #torch.save(net.state_dict(), DEFAULT_ENV_NAME + \"-best.dat\")\n",
    "                best_mean_reward = mean_reward\n",
    "                if best_mean_reward is not None:\n",
    "                    print(\"Best mean reward updated %.3f\" % (best_mean_reward))\n",
    "\n",
    "            if mean_reward > MEAN_REWARD_BOUND:\n",
    "                print(\"Solved in %d frames!\" % frame_idx)\n",
    "                break\n",
    "\n",
    "        if len(buffer) < replay_start_size:\n",
    "            continue\n",
    "\n",
    "        batch = buffer.sample(batch_size)\n",
    "        states, actions, rewards, dones, next_states = batch\n",
    "    \n",
    "        states_v = torch.tensor(states).to(device)\n",
    "        next_states_v = torch.tensor(next_states).to(device)\n",
    "        actions_v = torch.tensor(actions).to(device)\n",
    "        rewards_v = torch.tensor(rewards).to(device)\n",
    "        done_mask = torch.ByteTensor(dones).to(device)\n",
    "\n",
    "        ############################\n",
    "        #CONVERTED\n",
    "        #state_action_values = net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)\n",
    "        state_action_values = net(states_v).gather(1, actions_v.unsqueeze(-1).long()).squeeze(-1)\n",
    "\n",
    "        next_state_values = target_net(next_states_v).max(1)[0]\n",
    "\n",
    "        next_state_values[done_mask] = 0.0\n",
    "\n",
    "        next_state_values = next_state_values.detach()\n",
    "\n",
    "        expected_state_action_values = next_state_values * gamma + rewards_v\n",
    "\n",
    "        loss_t = nn.MSELoss()(state_action_values, expected_state_action_values)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss_t.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if frame_idx % sync_target_frames == 0:\n",
    "            target_net.load_state_dict(net.state_dict())\n",
    "       \n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MZPkszw66cmO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>Training ends at  2023-11-02 15:46:18.541311\n"
     ]
    }
   ],
   "source": [
    "print(\">>>Training ends at \",datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qNH2N64k3QRz"
   },
   "source": [
    "Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WKbcwfK321Hl"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-7d42400be924157c\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-7d42400be924157c\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tensorboard  --logdir=runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6p0jvxoC3m5W"
   },
   "source": [
    "## Using the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TLEfbkKl6AZV"
   },
   "outputs": [],
   "source": [
    "# import gym\n",
    "# import time\n",
    "# import numpy as np\n",
    "\n",
    "# import torch\n",
    "\n",
    "# import collections\n",
    "\n",
    "# DEFAULT_ENV_NAME = \"PongNoFrameskip-v4\"\n",
    "FPS = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4m0Vm4Yp91ZI"
   },
   "source": [
    "Tunning the image rendering in colab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kgpHXywd5SyZ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'apt-get' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyvirtualdisplay==0.2.*\n",
      "  Downloading PyVirtualDisplay-0.2.5-py2.py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: PyOpenGL==3.1.* in c:\\users\\james\\anaconda3\\lib\\site-packages (3.1.7)\n",
      "Collecting PyOpenGL-accelerate==3.1.*\n",
      "  Downloading PyOpenGL_accelerate-3.1.7-cp310-cp310-win_amd64.whl (318 kB)\n",
      "     -------------------------------------- 319.0/319.0 kB 2.8 MB/s eta 0:00:00\n",
      "Collecting EasyProcess\n",
      "  Downloading EasyProcess-1.1-py3-none-any.whl (8.7 kB)\n",
      "Installing collected packages: PyOpenGL-accelerate, EasyProcess, pyvirtualdisplay\n",
      "Successfully installed EasyProcess-1.1 PyOpenGL-accelerate-3.1.7 pyvirtualdisplay-0.2.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  python setup.py bdist_wheel did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [28 lines of output]\n",
      "  Using setuptools (version 65.6.3).\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-cpython-310\n",
      "  creating build\\lib.win-amd64-cpython-310\\Box2D\n",
      "  copying library\\Box2D\\Box2D.py -> build\\lib.win-amd64-cpython-310\\Box2D\n",
      "  copying library\\Box2D\\__init__.py -> build\\lib.win-amd64-cpython-310\\Box2D\n",
      "  creating build\\lib.win-amd64-cpython-310\\Box2D\\b2\n",
      "  copying library\\Box2D\\b2\\__init__.py -> build\\lib.win-amd64-cpython-310\\Box2D\\b2\n",
      "  running build_ext\n",
      "  building 'Box2D._Box2D' extension\n",
      "  swigging Box2D\\Box2D.i to Box2D\\Box2D_wrap.cpp\n",
      "  swig.exe -python -c++ -IBox2D -small -O -includeall -ignoremissing -w201 -globals b2Globals -outdir library\\Box2D -keyword -w511 -D_SWIG_KWARGS -o Box2D\\Box2D_wrap.cpp Box2D\\Box2D.i\n",
      "  Box2D\\Common\\b2Math.h(67) : Warning 302: Identifier 'b2Vec2' redefined by %extend (ignored),\n",
      "  Box2D\\Box2D_math.i(47) : Warning 302: %extend definition of 'b2Vec2'.\n",
      "  Box2D\\Common\\b2Math.h(158) : Warning 302: Identifier 'b2Vec3' redefined by %extend (ignored),\n",
      "  Box2D\\Box2D_math.i(168) : Warning 302: %extend definition of 'b2Vec3'.\n",
      "  Box2D\\Common\\b2Math.h(197) : Warning 302: Identifier 'b2Mat22' redefined by %extend (ignored),\n",
      "  Box2D\\Box2D_math.i(301) : Warning 302: %extend definition of 'b2Mat22'.\n",
      "  Box2D\\Common\\b2Math.h(271) : Warning 302: Identifier 'b2Mat33' redefined by %extend (ignored),\n",
      "  Box2D\\Box2D_math.i(372) : Warning 302: %extend definition of 'b2Mat33'.\n",
      "  Box2D\\Collision\\b2DynamicTree.h(44) : Warning 312: Nested union not currently supported (ignored).\n",
      "  Box2D\\Common\\b2Settings.h(144) : Warning 506: Can't wrap varargs with keyword arguments enabled\n",
      "  Box2D\\Common\\b2Math.h(91) : Warning 509: Overloaded method b2Vec2::operator ()(int32) effectively ignored,\n",
      "  Box2D\\Common\\b2Math.h(85) : Warning 509: as it is shadowed by b2Vec2::operator ()(int32) const.\n",
      "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for box2d-py\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Running setup.py install for box2d-py did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [30 lines of output]\n",
      "  Using setuptools (version 65.6.3).\n",
      "  running install\n",
      "  C:\\Users\\James\\anaconda3\\lib\\site-packages\\setuptools\\command\\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "    warnings.warn(\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-cpython-310\n",
      "  creating build\\lib.win-amd64-cpython-310\\Box2D\n",
      "  copying library\\Box2D\\Box2D.py -> build\\lib.win-amd64-cpython-310\\Box2D\n",
      "  copying library\\Box2D\\__init__.py -> build\\lib.win-amd64-cpython-310\\Box2D\n",
      "  creating build\\lib.win-amd64-cpython-310\\Box2D\\b2\n",
      "  copying library\\Box2D\\b2\\__init__.py -> build\\lib.win-amd64-cpython-310\\Box2D\\b2\n",
      "  running build_ext\n",
      "  building 'Box2D._Box2D' extension\n",
      "  swigging Box2D\\Box2D.i to Box2D\\Box2D_wrap.cpp\n",
      "  swig.exe -python -c++ -IBox2D -small -O -includeall -ignoremissing -w201 -globals b2Globals -outdir library\\Box2D -keyword -w511 -D_SWIG_KWARGS -o Box2D\\Box2D_wrap.cpp Box2D\\Box2D.i\n",
      "  Box2D\\Common\\b2Math.h(67) : Warning 302: Identifier 'b2Vec2' redefined by %extend (ignored),\n",
      "  Box2D\\Box2D_math.i(47) : Warning 302: %extend definition of 'b2Vec2'.\n",
      "  Box2D\\Common\\b2Math.h(158) : Warning 302: Identifier 'b2Vec3' redefined by %extend (ignored),\n",
      "  Box2D\\Box2D_math.i(168) : Warning 302: %extend definition of 'b2Vec3'.\n",
      "  Box2D\\Common\\b2Math.h(197) : Warning 302: Identifier 'b2Mat22' redefined by %extend (ignored),\n",
      "  Box2D\\Box2D_math.i(301) : Warning 302: %extend definition of 'b2Mat22'.\n",
      "  Box2D\\Common\\b2Math.h(271) : Warning 302: Identifier 'b2Mat33' redefined by %extend (ignored),\n",
      "  Box2D\\Box2D_math.i(372) : Warning 302: %extend definition of 'b2Mat33'.\n",
      "  Box2D\\Collision\\b2DynamicTree.h(44) : Warning 312: Nested union not currently supported (ignored).\n",
      "  Box2D\\Common\\b2Settings.h(144) : Warning 506: Can't wrap varargs with keyword arguments enabled\n",
      "  Box2D\\Common\\b2Math.h(91) : Warning 509: Overloaded method b2Vec2::operator ()(int32) effectively ignored,\n",
      "  Box2D\\Common\\b2Math.h(85) : Warning 509: as it is shadowed by b2Vec2::operator ()(int32) const.\n",
      "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: legacy-install-failure\n",
      "\n",
      "Encountered error while trying to install package.\n",
      "\n",
      "box2d-py\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for output from the failure.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gym[box2d]==0.17.*\n",
      "  Downloading gym-0.17.3.tar.gz (1.6 MB)\n",
      "     ---------------------------------------- 1.6/1.6 MB 1.4 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: scipy in c:\\users\\james\\anaconda3\\lib\\site-packages (from gym[box2d]==0.17.*) (1.10.1)\n",
      "Requirement already satisfied: numpy>=1.10.4 in c:\\users\\james\\anaconda3\\lib\\site-packages (from gym[box2d]==0.17.*) (1.25.0)\n",
      "Collecting pyglet<=1.5.0,>=1.4.0\n",
      "  Downloading pyglet-1.5.0-py2.py3-none-any.whl (1.0 MB)\n",
      "     ---------------------------------------- 1.0/1.0 MB 831.0 kB/s eta 0:00:00\n",
      "Collecting cloudpickle<1.7.0,>=1.2.0\n",
      "  Downloading cloudpickle-1.6.0-py3-none-any.whl (23 kB)\n",
      "Collecting box2d-py~=2.3.5\n",
      "  Downloading box2d-py-2.3.8.tar.gz (374 kB)\n",
      "     -------------------------------------- 374.5/374.5 kB 1.5 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: future in c:\\users\\james\\anaconda3\\lib\\site-packages (from pyglet<=1.5.0,>=1.4.0->gym[box2d]==0.17.*) (0.18.3)\n",
      "Building wheels for collected packages: box2d-py, gym\n",
      "  Building wheel for box2d-py (setup.py): started\n",
      "  Building wheel for box2d-py (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for box2d-py\n",
      "  Building wheel for gym (setup.py): started\n",
      "  Building wheel for gym (setup.py): finished with status 'done'\n",
      "  Created wheel for gym: filename=gym-0.17.3-py3-none-any.whl size=1654631 sha256=783bcdf5bb5a5d8a3041749640ce774b758478bfbf52a3aeebaf811b1ea20f1f\n",
      "  Stored in directory: c:\\users\\james\\appdata\\local\\pip\\cache\\wheels\\cc\\e4\\97\\f9097746896a5a5595e1477b95603324bf6dde572a89e88bc0\n",
      "Successfully built gym\n",
      "Failed to build box2d-py\n",
      "Installing collected packages: box2d-py, pyglet, cloudpickle, gym\n",
      "  Running setup.py install for box2d-py: started\n",
      "  Running setup.py install for box2d-py: finished with status 'error'\n"
     ]
    },
    {
     "ename": "EasyProcessError",
     "evalue": "start error <EasyProcess cmd_param=['Xvfb', '-help'] cmd=['Xvfb', '-help'] oserror=[WinError 2] The system cannot find the file specified return_code=None stdout=\"None\" stderr=\"None\" timeout_happened=False>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\easyprocess\\__init__.py:176\u001b[0m, in \u001b[0;36mEasyProcess.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 176\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpopen \u001b[38;5;241m=\u001b[39m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstdout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstderr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstderr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcwd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m oserror:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\subprocess.py:971\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize)\u001b[0m\n\u001b[0;32m    968\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[0;32m    969\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m--> 971\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    974\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    975\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    976\u001b[0m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    977\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    978\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    979\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    980\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m    981\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\subprocess.py:1440\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_gid, unused_gids, unused_uid, unused_umask, unused_start_new_session)\u001b[0m\n\u001b[0;32m   1439\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1440\u001b[0m     hp, ht, pid, tid \u001b[38;5;241m=\u001b[39m \u001b[43m_winapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCreateProcess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1441\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;66;43;03m# no special security\u001b[39;49;00m\n\u001b[0;32m   1442\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1443\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1444\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1445\u001b[0m \u001b[43m                             \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1446\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1447\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1448\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1449\u001b[0m     \u001b[38;5;66;03m# Child is launched. Close the parent's copy of those pipe\u001b[39;00m\n\u001b[0;32m   1450\u001b[0m     \u001b[38;5;66;03m# handles that only the child should have open.  You need\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1453\u001b[0m     \u001b[38;5;66;03m# pipe will not close when the child process exits and the\u001b[39;00m\n\u001b[0;32m   1454\u001b[0m     \u001b[38;5;66;03m# ReadFile will hang.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mEasyProcessError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 12\u001b[0m\n\u001b[0;32m      8\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip install gym[box2d]==0.17.*\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyvirtualdisplay\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m _display \u001b[38;5;241m=\u001b[39m \u001b[43mpyvirtualdisplay\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDisplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvisible\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1400\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m900\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m _ \u001b[38;5;241m=\u001b[39m _display\u001b[38;5;241m.\u001b[39mstart()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyvirtualdisplay\\display.py:34\u001b[0m, in \u001b[0;36mDisplay.__init__\u001b[1;34m(self, backend, visible, size, color_depth, bgcolor, use_xauth, check_startup, randomizer, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackend \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxvfb\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisplay_class\u001b[49m(\n\u001b[0;32m     35\u001b[0m     size\u001b[38;5;241m=\u001b[39msize,\n\u001b[0;32m     36\u001b[0m     color_depth\u001b[38;5;241m=\u001b[39mcolor_depth,\n\u001b[0;32m     37\u001b[0m     bgcolor\u001b[38;5;241m=\u001b[39mbgcolor,\n\u001b[0;32m     38\u001b[0m     randomizer\u001b[38;5;241m=\u001b[39mrandomizer,\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     40\u001b[0m AbstractDisplay\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, use_xauth\u001b[38;5;241m=\u001b[39muse_xauth, check_startup\u001b[38;5;241m=\u001b[39mcheck_startup, randomizer\u001b[38;5;241m=\u001b[39mrandomizer)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyvirtualdisplay\\display.py:53\u001b[0m, in \u001b[0;36mDisplay.display_class\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m XephyrDisplay\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# TODO: check only once\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_installed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyvirtualdisplay\\xvfb.py:40\u001b[0m, in \u001b[0;36mXvfbDisplay.check_installed\u001b[1;34m(cls)\u001b[0m\n\u001b[0;32m     38\u001b[0m p\u001b[38;5;241m.\u001b[39menable_stdout_log \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     39\u001b[0m p\u001b[38;5;241m.\u001b[39menable_stderr_log \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\easyprocess\\__init__.py:147\u001b[0m, in \u001b[0;36mEasyProcess.call\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run command with arguments. Wait for command to complete.\u001b[39;00m\n\u001b[0;32m    137\u001b[0m \n\u001b[0;32m    138\u001b[0m \u001b[38;5;124;03msame as:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    144\u001b[0m \n\u001b[0;32m    145\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 147\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mwait(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    149\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_alive():\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\easyprocess\\__init__.py:186\u001b[0m, in \u001b[0;36mEasyProcess.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    184\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOSError exception: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, oserror)\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moserror \u001b[38;5;241m=\u001b[39m oserror\n\u001b[1;32m--> 186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m EasyProcessError(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_started \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    188\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocess was started (pid=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpid)\n",
      "\u001b[1;31mEasyProcessError\u001b[0m: start error <EasyProcess cmd_param=['Xvfb', '-help'] cmd=['Xvfb', '-help'] oserror=[WinError 2] The system cannot find the file specified return_code=None stdout=\"None\" stderr=\"None\" timeout_happened=False>"
     ]
    }
   ],
   "source": [
    "# Taken from \n",
    "# https://towardsdatascience.com/rendering-openai-gym-envs-on-binder-and-google-colab-536f99391cc7\n",
    "\n",
    "!apt-get install -y xvfb x11-utils\n",
    "\n",
    "!pip install pyvirtualdisplay==0.2.* \\\n",
    "             PyOpenGL==3.1.* \\\n",
    "             PyOpenGL-accelerate==3.1.*\n",
    "\n",
    "!pip install gym[box2d]==0.17.*\n",
    "\n",
    "import pyvirtualdisplay\n",
    "\n",
    "_display = pyvirtualdisplay.Display(visible=False, size=(1400, 900))\n",
    "_ = _display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BvN4S8R53mJI"
   },
   "outputs": [],
   "source": [
    "# Taken (partially) from \n",
    "# https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On/blob/master/Chapter06/03_dqn_play.py\n",
    "\n",
    "\n",
    "model='PongNoFrameskip-v4-best.dat'\n",
    "record_folder=\"video\"  \n",
    "visualize=True\n",
    "\n",
    "env = make_env(DEFAULT_ENV_NAME)\n",
    "if record_folder:\n",
    "        env = gym.wrappers.Monitor(env, record_folder, force=True)\n",
    "net = DQN(env.observation_space.shape, env.action_space.n)\n",
    "net.load_state_dict(torch.load(model, map_location=lambda storage, loc: storage))\n",
    "\n",
    "state = env.reset()\n",
    "total_reward = 0.0\n",
    "\n",
    "while True:\n",
    "        start_ts = time.time()\n",
    "        if visualize:\n",
    "            env.render()\n",
    "        state_v = torch.tensor(np.array([state], copy=False))\n",
    "        q_vals = net(state_v).data.numpy()[0]\n",
    "        action = np.argmax(q_vals)\n",
    "        \n",
    "        state, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "        if visualize:\n",
    "            delta = 1/FPS - (time.time() - start_ts)\n",
    "            if delta > 0:\n",
    "                time.sleep(delta)\n",
    "print(\"Total reward: %.2f\" % total_reward)\n",
    "\n",
    "if record_folder:\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='citations'></a>\n",
    "[1] Sutton, R. S., &amp; Barto, A. G. (2020). Reinforcement learning: An introduction. The MIT Press. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DRL_15_16_17_DQN_Pong.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
